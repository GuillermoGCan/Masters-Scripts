7!
recurS <- function(x){
for (i in x:1){
res = res * i
}
}
recurS(5)
recurS(5)
recurS <- function(x){
res=1
for (i in x:1){
res = res * i
}
}
recurS(5)
recurS(5)
recurS(5)
##
recurS <- function(x){
res=1
for (i in x:1){
res = res * i
}
return(res)
}
recurS(5)
recurS(100)
##
FactorSIter <- function(x){
res=1
for (i in x:1){
res = res * i
}
return(res)
}
FactorSIter(100)
remove(recurS())
remove(recurS)
#Recursión
FactorSRecurs <- function (input) {
if (input == 0 || input ==1){
return (1)
} else{
return(input * FactorSRecurs(input-1))
}
}
FactorSRecurs(5)
FactorSRecurs(100)
Carseats <- read.csv("Carseats.csv")
Carseats <- Carseats[complete.cases(Carseats),]
Carseats$High <- ifelse(Carseats$Sales <= 8, "No", "Yes")
class(Carseats$High)
# Como la variable es caracter, se tiene que hacer factor o variable categórica
Carseats$High <- as.factor(Carseats$High)
class(Carseats$High)
?tree
Carseats$High <- as.factor(Carseats$High)
Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc)
Carseats$Urban <- as.factor(Carseats$Urban)
Carseats$US <- as.factor(Carseats$US)
tree.carseats = tree(High~.-Sales, Carseats)
install.packages("ISLR")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("tree")
install.packages("MASS")
install.packages("randomForest")
library("ISLR")
library("rpart")
library("rpart.plot")
library("tree")
library("MASS")
library("randomForest")
Carseats = read.csv("Carseats.csv")
Carseats = Carseats[complete.cases(Carseats),]
Carseats$High = ifelse(Carseats$Sales <=8, "No", "Yes")
class(Carseats$High)
Carseats$High = as.factor(Carseats$High)
Carseats$ShelveLoc = as.factor(Carseats$ShelveLoc)
Carseats$Urban = as.factor(Carseats$Urban)
Carseats$US = as.factor(Carseats$US)
Carseats$High = as.factor(Carseats$High)
tree.carseats = tree(High~.-Sales, Carseats)
summary(tree.carseats)
#Error de clasificaci?n del 9%
plot(tree.carseats)
text(tree.carseats, pretty=0)
#Imprimir resultados como texto
tree.carseats
plot(tree.carseats)
text(tree.carseats, pretty=0)
sizeCars = nrow(Carseats)
train = sample(1:sizeCars, sizeCars/2)
Carseats.train = Carseats[train,]
Carseats.test = Carseats[-train,]
tree.carsets.train = tree(High~.-Sales, Carseats, subset=train)
summary(tree.carseats.train)
plot(tree.carseats.train)
text(tree.carseats.train, pretty = 0)
Carseats <- read.csv("Carseats.csv")
Carseats <- Carseats[complete.cases(Carseats),]
Carseats$High <- ifelse(Carseats$Sales <=8, "No", "Yes")
class(Carseats$High)
Carseats$High <- as.factor(Carseats$High)
Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc)
Carseats$Urban <- as.factor(Carseats$Urban)
Carseats$US <- as.factor(Carseats$US)
Carseats$High <- as.factor(Carseats$High)
tree.carseats = tree(High~.-Sales, Carseats)
summary(tree.carseats)
# Error de clasificación del 9%
plot(tree.carseats)
text(tree.carseats, pretty=0)
# El predictor más importante es shelve location
#Imprimir resultados como texto
tree.carseats
# Evaluar una muestra cruzada.
sizeCars = nrow(Carseats)
train = sample(1:sizeCars, sizeCars/2)
Carseats.train <- Carseats[train,]
Carseats.test <- Carseats[-train,]
tree.carseats.train = tree(High~.-Sales, Carseats, subset=train)
summary(tree.carseats.train)
plot(tree.carseats.train)
text(tree.carseats.train, pretty=0)
# Usemos el modelo calibrado o ajustado con la base de entrenamiento, para predecir sobre
# la base de prueba.
# Si mi modelo predice correctamente el mismo 91%, entonces no estoy haciendo overfitting.
## Training y Testing
tree.predict.test <- predict(tree.carseats.train, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
## Vamos a poder nuestro árbol
################################################################################
boston <- read.csv("boston.csv")
## Crossvalidation
sizeBoston = nrow(Boston)
train = sample(1:sizeBoston, sizeBoston/2)
Boston[train,]
tree.boston <- tree(medv~., Boston, subset=train)
plot(tree.boston)
text(tree.boston, pretty=0)
plot(tree.carseats)
text(tree.carseats, pretty=0)
#Imprimir resultados como texto
tree.carseats
table(tree.predict.test, Carseats$High[-train])
## Training y Testing
tree.predict.test <- predict(tree.carseats.train, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
table(tree.predict.test, Carseats$High[-train])
## Training y Testing
tree.predict.test <- predict(tree.carseats.train, Carseats.test, type="class")
table(tree.predict.test, Carseats$High[-train])
library("ISLR")
library("rpart")
library("rpart.plot")
library("tree")
library("MASS")
library("randomForest")
Carseats <- read.csv("Carseats.csv")
Carseats <- Carseats[complete.cases(Carseats),]
Carseats$High <- ifelse(Carseats$Sales <=8, "No", "Yes")
class(Carseats$High)
Carseats$High <- as.factor(Carseats$High)
Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc)
Carseats$Urban <- as.factor(Carseats$Urban)
Carseats$US <- as.factor(Carseats$US)
Carseats$High <- as.factor(Carseats$High)
tree.carseats = tree(High~.-Sales, Carseats)
summary(tree.carseats)
# Error de clasificación del 9%
plot(tree.carseats)
text(tree.carseats, pretty=0)
# El predictor más importante es shelve location
#Imprimir resultados como texto
tree.carseats
# Evaluar una muestra cruzada.
sizeCars = nrow(Carseats)
train = sample(1:sizeCars, sizeCars/2)
Carseats.train <- Carseats[train,]
Carseats.test <- Carseats[-train,]
tree.carseats.train = tree(High~.-Sales, Carseats, subset=train)
summary(tree.carseats.train)
plot(tree.carseats.train)
text(tree.carseats.train, pretty=0)
# Usemos el modelo calibrado o ajustado con la base de entrenamiento, para predecir sobre
# la base de prueba.
# Si mi modelo predice correctamente el mismo 91%, entonces no estoy haciendo overfitting.
## Training y Testing
tree.predict.test <- predict(tree.carseats.train, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
## Vamos a poder nuestro árbol
################################################################################
boston <- read.csv("boston.csv")
## Crossvalidation
sizeBoston = nrow(Boston)
train = sample(1:sizeBoston, sizeBoston/2)
Boston[train,]
tree.boston <- tree(medv~., Boston, subset=train)
plot(tree.boston)
text(tree.boston, pretty=0)
cv.tree = cv.tree(tree.carseats.train, FUN=prune.misclass)
names(cv.tree)
cv.tree
par(mfrow=c(1,2))
plot(cv.tree$size, cv.tree$dev, type="b")
plot(cv.tree$k,cv.tree$dev, type"b")
plot(cv.tree$size, cv.tree$dev, type="b")
plot(cv.tree$size, cv.tree$dev, type="b")
plot(cv.tree$k,cv.tree$dev, type="b")
par(mfrow=c(1,2))
plot(cv.tree$size, cv.tree$dev, type="b")
plot(cv.tree$k,cv.tree$dev, type="b")
pruned9 <- prune.misclass(tree.carseats.train, best=9)
plot(pruned9)
text(pruned9)
tree.predict.test <- predict(pruned9, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
table(tree.predict.test, Carseats$High[-train])
library("ISLR")
library("rpart")
library("rpart.plot")
library("tree")
library("MASS")
library("randomForest")
table(tree.predict.test, Carseats$High[-train])
summary(tree.predict.test)
tree.predict.test <- predict(pruned9, Carseats.test, type="class")
source('C:/Users/Guillermo GC/Dropbox/TEC MTY MPE/2do Trimestre/Ciencia de datos/Sesión 10/lab_trees.R', encoding = 'UTF-8', echo=TRUE)
table(tree.predict.test, Carseats$High[-train])
table(tree.predict.test, Carseats$High[-train])
tree.predict.test <- predict(pruned9, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
rm(list=ls())
Carseats <- read.csv("Carseats.csv")
Carseats <- Carseats[complete.cases(Carseats),]
Carseats$High <- ifelse(Carseats$Sales <=8, "No", "Yes")
class(Carseats$High)
Carseats$High <- as.factor(Carseats$High)
Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc)
Carseats$Urban <- as.factor(Carseats$Urban)
Carseats$US <- as.factor(Carseats$US)
Carseats$High <- as.factor(Carseats$High)
tree.carseats = tree(High~.-Sales, Carseats)
summary(tree.carseats)
# Error de clasificación del 9%
plot(tree.carseats)
text(tree.carseats, pretty=0)
# El predictor más importante es shelve location
#Imprimir resultados como texto
tree.carseats
# Evaluar una muestra cruzada.
sizeCars = nrow(Carseats)
train = sample(1:sizeCars, sizeCars/2)
Carseats.train <- Carseats[train,]
Carseats.test <- Carseats[-train,]
tree.carseats.train = tree(High~.-Sales, Carseats, subset=train)
summary(tree.carseats.train)
plot(tree.carseats.train)
text(tree.carseats.train, pretty=0)
# Usemos el modelo calibrado o ajustado con la base de entrenamiento, para predecir sobre
# la base de prueba.
# Si mi modelo predice correctamente el mismo 91%, entonces no estoy haciendo overfitting.
## Training y Testing
tree.predict.test <- predict(tree.carseats.train, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
## Vamos a poder nuestro árbol
cv.tree = cv.tree(tree.carseats.train, FUN=prune.misclass)
names(cv.tree)
cv.tree
par(mfrow=c(1,2))
plot(cv.tree$size,cv.tree$dev,type="b")
plot(cv.tree$k,cv.tree$dev,type="b")
## Prueba la poda con varios numero nodos y vuelve a utilizar tu base de pruebas
pruned9 <-  prune.misclass(tree.carseats.train, best=8)
plot(pruned9)
text(pruned9)
tree.predict.test <- predict(pruned9, Carseats.test, type="class")
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
################################################################################
boston <- read.csv("boston.csv")
## Crossvalidation
sizeBoston = nrow(Boston)
train = sample(1:sizeBoston, sizeBoston/2)
Boston[train,]
tree.boston <- tree(medv~., Boston, subset=train)
plot(tree.boston)
text(tree.boston, pretty=0)
table(tree.predict.test, Carseats$High[-train])
summary(tree.predict.test)
table(tree.predict.test, Carseats$High[-train])
length(tree.predict.test)
length(Carseats$High[-train])
table(tree.predict.test, Carseats.test$High)
yhat <- predict(tree.boston, Boston[-train])
yhat <- predict(tree.boston, Boston[-train,])
summary(tree.boston)
randomForest(medv~. data=Boston, subset=train, ntree=50, mtry=10) #para el bagging,
randomForest(medv~, data=Boston, subset=train, ntree=50, mtry=13) #para el bagging,
randomForest(medv~., data=Boston, subset=train, ntree=50, mtry=13) #para el bagging,
bag.boston <-
randomForest(medv~., data=Boston, subset=train, ntree=50, mtry=13) #para el bagging,
library("ISLR")
library("rpart")
library("rpart.plot")
library("tree")
library("MASS")
library("randomForest")
Carseats <- read.csv("Carseats.csv")
View(Carseats)
View(Carseats)
Carseats <- Carseats[complete.cases(Carseats),]
Carseats$High <- ifelse(Carseats$Sales <=8, "No", "Yes")
class(Carseats$High)
Carseats$High <- as.factor(Carseats$High)
Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc)
Carseats$Urban <- as.factor(Carseats$Urban)
Carseats$US <- as.factor(Carseats$US)
Carseats$High <- as.factor(Carseats$High)
tree.carseats = tree(High~.-Sales, Carseats)
summary(tree.carseats)
(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
